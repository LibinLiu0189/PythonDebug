### 1.0 概念
池化:把很多数据用最大值或者平均值代替。目的是降低数据量。
卷积:把数据通过一个卷积核变化成特征，便于后面的分离。计算方式与信号系统中的相同。
[如何理解卷积神经网络（CNN）中的卷积和池化？](https://www.zhihu.com/question/49376084)
### 2.0 统计不变性
即基本上不会随时间或空间改变的事物。

- 平移不变性

如果人们选择图像中的连续范围(照片中猫咪)作为池化区域，并且只是池化相同(重复)的隐藏单元产生的特征，那么，这些池化单元就具有平移不变性 (`translation invariant`)。

这就意味着即使图像经历了一个小的平移之后，依然会产生相同的 (池化的) 特征。

在很多任务中 (例如物体检测、声音识别)，我们都更希望得到具有平移不变性的特征，因为即使图像经过了平移，样例(图像)的标记仍然保持不变。

- 权重共享

当知道两个输入可能包含相同类型的信息时，通用它们的权重，并利用这些输入共同训练权重。

### 3.0 卷积网络
CovNet是一种空间上共享参数的神经网络。

![cov_neural_net_1.png](https://i.imgur.com/SPnXU7y.png)

上图，在最右端定义一个分类器，所有空间信息被压缩成一个表示，仅映射到图片内容的参数被保留。

##### 3.1.1 CNN层次相关概念

![cov_neural_net_2.png](https://i.imgur.com/8G70ndB.png)

----------

##### 3.1.2 CNN学习实例图解

![cov_neural_net_3.png](https://i.imgur.com/iqyppiO.jpg)

CNN可能有几层网络，每个层可能捕获对象抽象层次中的不同级别。
- 第一层是抽象层次的最底级，CNN 一般把图片中的较小的部分识别成简单的形状，例如水平、竖直的直线，简单的色块。
- 下一层将会上升到更高的抽象层次，一般会识别更复杂的概念，例如形状（线的组合），
- 以此类推直至最终识别整个物体，例如狗。

再次强调，CNN 是**自主学习**。我们不需要告诉 CNN 去寻找任何直线、曲线、鼻子、毛发等等。CNN 从训练集中学习并发现金毛巡回犬值得寻找的特征。

### 4.0 滤波器 Filters
#### 4.1 分解一张图片

CNN 的第一步是把图片分成小块。我们通过选取一个给定宽度和高度的滤波器来实现这一步。

滤波器会照在图片的小块`patch`（图像区块）上。这些`patch`的大小与滤波器一样大。

![cov_neural_net_4.png](https://i.imgur.com/8SAKOp5.png)

滤波器滑动的间隔被称作 `stride`（步长）。这是你可以调节的一个超参数。增大 `stride` 值后，会减少每层总 `patch` 数量，因此也减小了模型大小。通常这也会降低图像精度。

#### 4.2 滤波器深度 Filter Depth
通常都会有多余一个滤波器，不同滤波器提取一个 `patch` 的不同特性。例如，一个滤波器寻找特定颜色，另一个寻找特定物体的特定形状。**卷积层滤波器的数量**被称为滤波器深度。

###### 每个 patch 连接多少神经元？

这取决于滤波器的深度，如果深度是 `k`，我们把每个 `patch` 与下一层的 `k` 个神经元相连。这样下一层的高度就是 `k`，如下图所示。实际操作中，`k`是一个我们可以调节的超参数，大多数的 `CNNs` 倾向于选择相同的起始值。

![cov_neural_net_5.png](https://i.imgur.com/0TXnm1f.png)

一个 `patch` 连接有**多个神经元**可以保证我们的 `CNNs` 学会**提取任何它觉得重要的特征**。
记住，`CNN` 并没有被规定寻找特定特征。与之相反，它自我学习什么特征值得注意。

#### 4.3 卷积续
全链接层是一个标准的，非卷积层。它的输入与所有的输出神经相连，也被称为 dense 层