## 1.0 第一周 循环神经网络(Recurrent Neural Networks,RNN)
### 1.1 为什么选择序列模型
在DNN和CNN中，训练样本的输入和输出是比较的确定的。但是有一类问题DNN和CNN不好解决，就是训练样本输入是连续的序列,且序列的长短不一，比如基于时间的序列：一段段连续的语音，一段段连续的手写文字。这些序列比较长，且长度不一，比较难直接的拆分成一个个独立的样本来通过DNN/CNN进行训练。

对于这类问题循环神经网络(Recurrent Neural Networks,RNN)比较擅长处理。

![sequence_data_1.png](https://i.imgur.com/PEiDYwg.png)
</br>上图表示哪些问题中使用到了序列数据

### 1.2 数学符号
#### 命名实体识别
![sequence_data_2.png](https://i.imgur.com/yEjuY6M.png)

1. X<sup>(i)<\t></sup>:训练样本i的序列中第t个元素。
1. T<sub>x</sub><sup>(i)</sup>:第i个训练样本的输入序列长度
1. y<sup>(i)<\t></sup>:训练样本i的序列中第t个元素的标签。
1. T<sub>y</sub><sup>(i)</sup>:第i个训练样本的输出序列的长度。

序列中的每个元素有相应的标签，一般需要先建立一个包含序列中所有类型元素的字典。然后根据单词在字典中的位置，用one-hot向量来表示该单词的标签。如下图：

![sequence_data_3.png](https://i.imgur.com/PPq5yMa.png)

#### 对于不在词表中的单词？

创建一个新的标记，也就是一个叫做Unknown Word的伪造单词用作为标记，来表示不在词表中的单词。

### 1.3 循环神经网络模型
#### 标准的神经网络

![sequence_data_4.png](https://i.imgur.com/W5fdqFN.png)

标准的神经网络面临的问题:
1.inputs outputs can be different lengths in different examples.

输入和输出数据在不同例子中可以有不同的长度，不是所有例子都有想用的输入长度T<sub>x</sub>或同样的输出长度T<sub>y</sub>。

2.doesn't share features learned across different positions of text.

不共享从文本不同位置上学到的特征。

#### RNN Forward Propagation
![RNN_1.png](https://i.imgur.com/JJGNQGW.png)

上图最右侧的展开为左侧的序列模型。
其中，元素x<sup><\t></sup>输入对应时间步的隐藏层同时，该隐藏层也会接收上一时间步的隐藏层激活a<sup><\t-1></sup>，其中a<sup><\0></sup>一般初始化位零向量。一个时间步输出一个对应的预测结果yhat<sup><\t></sup>，输入、激活、输出有对应的参数W<sub>ax</sub>、W<sub>aa</sub>、W<sub>y</sub>。

a<sup><\0></sup> = 0</br>
a<sup><\t></sup> = g1(W<sub>aa</sub>a<sup><\t-1></sup> + W<sub>ax</sub>x<sup><\t></sup> + b<sub>a</sub>)</br>
yhat<sup><\t></sup> = g2(W<sub>y</sub>a<sup><\t></sup> + b<sub>y</sub>)</br>

b<sub>a</sub>、b<sub>y</sub>是两个偏差参数，

激活函数g1通常选择tanh、ReLU；g2的选择取决于需要的输出类型，可选sigmoid或者Softmax。

上述公式简化：

W<sub>a</sub> = [W<sub>aa</sub>,W<sub>ax</sub>]</br>
a<sup><\t></sup> = g1(W<sub>a</sub>[a<sup><\t-1></sup> + x<sup><\t></sup>]<sup>T</sup> + b<sub>a</sub>)</br>

### 1.4 通过时间的反向传播
RNN反向传播示意图：

![back_propagation_1.jpg](https://i.imgur.com/S1MpH40.jpg)

### 1.5 不同类型的循环神经网络
![RNN_types_1.png](https://i.imgur.com/CrdizAZ.png)

### 1.6 语言模型和序列模型
语言模型所做的就是，它会告诉你某个特定的句子它出现的概率是多少。

语料库：很长的或者说数量众多的英文句子组成的文本。

#### 如何建立一个语言模型？
第一，标识化

1. 将这个句子标记化，意思是建立一个字典，
2. 将每个单词都转换成对应的one-hot向量，也就是字典中的索引
3. 定义句子的结尾，一般的做法就是增加一个额外的标记，叫做EOS(表示句子的结尾)
4. 训练集集中的单词不在字典里，将这个词替换成UNK(代表未知词的标志)

完成标识化的过程后，意味着输入的句子都映射到各个标志上，或者说字典中的各个词上。

下一步我们要构建一个RNN来构建这些序列的概率模型。

![RNN_2.png](https://i.imgur.com/lj2WBcr.png)

#### 损失函数
![RNN_cost_1.png](https://i.imgur.com/Xo9H1qP.png)

成本函数：

![RNN_cost_2.png](https://i.imgur.com/SL5CWLV.png)

yhat<sup><\t></sup>：是通过softmax预测出的字典中每一个词作为第一个词出现的概率。

y<sup><\t></sup>：上一层的输出概率。

### 1.7 对新序列采样(Sampling novel sequences)
训练一个模型后，想了解模型学到了什么，一种非正式的方法就是进行一次新序列采样。
一个序列模型模拟了任意特定单词序列的概率，要做的就是对这些概率分布进行采样来生成一个新的单词序列。

![RNN_sampling_1.png](https://i.imgur.com/b5xcHlc.png)

停止采样：
1. 一直进行采样直到得到EOS标识，这代表着已经抵达结尾，可以停止采样了。
2. 另一种情况是，如果字典中没有这个词，可以决定从20个或100个或其他个单词进行采样，然后一直将采样进行下去直到达到所设定的时间步。

不过这种过程有时候会产生一些未知标识：
1. 如果要确保你的算法不会输出这种标识，能做的一件事就是拒绝采样过程中产生任何未知的标识，一旦出现就继续在剩下的词中进行重采样，直到得到一个不是未知标识的词。
2. 如果不介意有未知标识产生的话，也可以完全不管它们。

#### 基于字符的语言模型
![RNN_char_1.png](https://i.imgur.com/Ab0J1Fo.png)

优点:不必担心会出现未知的标识。</br>
缺点:最后会得到太长的序列。

基于字符的语言模型与基于词汇的模型比较:

基于字符的语言模型在捕捉句子中的依赖关系也就是句子较前部分如何影响较后部分不如基于词汇的语言模型那样可以捕捉长范围的关系，并且基于字符的语言模型训练起来计算成本比较高昂。

### 1.8 循环神经网络的梯度消失
#### 深度神经网络梯度消失
一个很深很深的网络，100层，甚至更深，对这个网络从左到右做前向传播然后再反向传播。如果这是个很深的神经网络，从输出得到的梯度很难传播回去，很难影响靠前层的权重，很难影响前面层（编号5所示的层）的计算。

#### 循环神经网络的梯度消失
![RNN_vanishing_gradients_1.png](https://i.imgur.com/z6Vb1Zz.png)

基本的RNN模型不擅长处理长期依赖的问题:

如上图，句中后面的动词用“was”还是“were”取决于前面的名词“cat”是单数还是复数。但是目前见到的基本的RNN模型（上图编号3所示的网络模型），不擅长捕获这种长期依赖效应。

原因:一般的循环神经网络也会出现类似于深度神经网络中的梯度消失问题，上图中输出yhat\<i\>主要受到yhat\<i\>附近的影响，而受到早期输入序列的影响。
#### 梯度爆炸的解决方法
使用梯度修剪。梯度修剪的意思就是观察梯度向量，如果它大于某个阈值，缩放梯度向量，保证它不会太大，这就是通过一些最大值来修剪的方法。

### 1.9 GRU单元(Gated Recurrent Units,门控循环单元)
GRU单元改变了RNN的隐藏层，使其可以更好地捕捉深层连接，并改善了梯度消失问题。

一般的RNN单元：

![RNN_3.png](https://i.imgur.com/9pG4fvJ.png)

GRU单元将会有个新的变量称为，代表细胞（cell），即记忆细胞（下图编号1所示）。记忆细胞的作用是提供了记忆的能力，比如说一只猫是单数还是复数，所以当它看到之后的句子的时候，它仍能够判断句子的主语是单数还是复数。

于是在时间t处：有记忆细胞c<sup><\t></sup>，然后我们看的是，GRU实际上输出了激活值a<sup><\t></sup>，c<sup><\t></sup>=a<sup><\t></sup>。于是我们想要使用不同的符号和来表示记忆细胞的值和输出的激活值，即使它们是一样的。

![RNN_GRU_1.png](https://i.imgur.com/HWRC12c.png)

![RNN_GRU_3.png](https://i.imgur.com/btR5Ytd.png)

c~的计算中以tanh作为激活函数，使用sigmoid作为激活函数得到的Γ<sub>u</sub>值将在0到1的范围内。

![RNN_GRU_2.png](https://i.imgur.com/S4qaeMl.png)

Γ<sub>u</sub>，u代表更新门，这是一个0到1之间的值。

![RNN_GRU_4.png](https://i.imgur.com/ZhFweb1.png)

当从左到右扫描句子的时候，因为sigmoid的值，门很容易取到0值，Γ<sub>u</sub>=0，意思是不更新c<sup><\t></sup>，也不要忘记这个值是什么，

如果Γ<sub>u</sub>=0，c<sup><\t></sup>=c<sup><\t-1></sup>，c<sup><\t></sup>等于旧值，即使经过很多时间步，这就是缓解土地消失问题的关键。

Γ<sub>u</sub>=1，则更新这个值。

![RNN_GRU_5.png](https://i.imgur.com/4FCQ8XU.png)

c<sup><\t></sup>可以是一个向量。如果c<sup><\t></sup>是100维，那么c~<sup><\t></sup>、Γ<sub>u</sub>也是100维，Γ<sub>u</sub>里面的值几乎都是0或1。实际应用中，Γ<sub>u</sub>并不会整的等于0或1，有时候它是0到1的一个中间值，可以选择保存一些比特不变，而去更新其它比特。


#### Full GRU
对于完整的GRU单元我要做的一个改变就是在计算的第一个式子中给记忆细胞的新候选值加上一个新的项。

![RNN_FullGRU_6.png](https://i.imgur.com/aZjd0Lr.jpg)

![RNN_FullGRU_7.png](https://i.imgur.com/YA0PScI.png)

Γ<sub>r</sub>表示下一个c<sup><\t></sup>的候选值c~<sup><\t></sup>跟c<sup><\t-1></sup>有多大的关联性。

### 1.10 长短期记忆(long short term memory,LSTM)
更新门：Γ<sub>u</sub>，更新参数W<sub>u</sub>；</br>
遗忘门(the forget gate)：Γ<sub>f</sub>，更新参数W<sub>f</sub>；</br>
输出门：Γ<sub>o</sub>，更新参数W<sub>o</sub>；</br>

![RNN_LSTM_1.png](https://i.imgur.com/0RxjW60.png)

c<sup><\t></sup>有选择权取维持旧的c<sup><\t-1></sup>或者加上新的值c~<sup><\t></sup>，所以单独使用更新门Γ<sub>u</sub>、遗忘门Γ<sub>f</sub>。

#### 1.10.01 LSTM网络
![RNN_LSTM_2.png](https://i.imgur.com/6UIhUsp.png)

上图红线，只要正确的设置了遗忘门和更新门，LSTM是相当容易把c<sup><\0></sup>的值一直往下传递到右边。这就是为什么LSTM和GRU非常擅长于长时间记忆某个值，对于存在记忆细胞中的某个值，即是经过很长很长的时间步。

#### 1.10.02 窥视孔连接

最常用LSTM版本可能是门值不仅取决于a<sup><\t-1></sup>和x<sup><\t></sup>，有时候可以偷窥下c<sup><\t-1></sup>的值，这叫做“窥视孔连接”(peephole connection)。“**窥视孔连接**”其实的意思是门值不仅取决于a<sup><\t-1></sup>和x<sup><\t></sup>，也取决于上一个记忆细胞的值c<sup><\t-1></sup>。然后“窥视孔连接”就可以结合Γ<sub>u</sub>、Γ<sub>f</sub>、Γ<sub>o</sub>来计算。

#### 1.10.03 LSTM反向传播
门求偏导：

![lstm_1.png](https://i.imgur.com/KLscNPG.png)

参数求偏导：

![lstm_2.png](https://i.imgur.com/Whdmq6X.png)

为了计算db<sub>f</sub>、db<sub>u</sub>、db<sub>c</sub>、db<sub>o</sub>需要各自对dΓ<sub>r</sub><sup><\t></sup>、dΓ<sub>u</sub><sup><\t></sup>、dΓ<sub>o</sub><sup><\t></sup>、dc~<sup><\t></sup>求和。

最后，计算隐藏状态、记忆状态和输入的偏导数：

![lstm_3.png](https://i.imgur.com/y46lWLi.png)

#### 1.10.04 GRU & LSTM
GRU的优点是这是个更加简单的模型，所以更容易创建一个更大的网络，而且它只有两个门，在计算性上也运行得更快，然后它可以扩大模型的规模。

LSTM更加强大和灵活。
### 1.11 双向循环神经网络(Bidirectional RNN,BRNN)
双向循环神经网络(Bidirectional RNN,BRNN)产生的背景：

![BRNN_1.png](https://i.imgur.com/XuTdJHM.png)

在命名实体识别中，单向的RNN不能判断上图第三个词是人名还是Teddy bears。

![BRNN_2.png](https://i.imgur.com/ugVwsMJ.png)

上图紫色序列是正向传播，绿色是反向循环。给定一个序列x<sup><1></sup>到x<sup><4></sup>，首先计算前向的a-><sup><1></sup>，然后计算前向的a-><sup><2></sup>，接着a-><sup><3></sup>、a-><sup><4></sup>。而反向序列从计算a<-<sup><4></sup>开始，反向进行，计算反向的a<-<sup><3></sup>，计算完了a<-<sup><3></sup>就可以用这些激活值计算反向的a<-<sup><2></sup>，然后是反向的a<-<sup><1></sup>，把这些激活值计算完了就可以计算预测结果了。这样时间x<sup><3></sup>不经输入过去的信息，还有现在的信息。同时考虑了过去和未来。

![BRNN_3.png](https://i.imgur.com/SawIo7O.png)

这些基本单元不仅仅是标准的RNN单元，也可以是GRU、LSTM单元。

双向RNN网络模型的缺点：需要完整的数据的序列，才能预测任意位置。

### 1.11 深层循环神经网络(Deep RNNs)
深层循环神经网络(Deep RNNs):把RNN的多个层堆叠在一起构建更深的模型。

![DRNN_1.png](https://i.imgur.com/Tjwu0Q6.png)

x<sup>[l]<\t></sup>:表示第l层的激活值，<\t>表示第t个时间点。

a<sup>[2]<3></sup> = g(W<sub>a</sub><sup>[2]</sup>[a<sup>[2]<2></sup>,a<sup>[1]<3></sup>] + b<sub>a</sub><sup>[2]</sup>)

a<sup>[2]<3></sup>的激活值从a<sup>[2]<2></sup>和a<sup>[1]<3></sup>输入。