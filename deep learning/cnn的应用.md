### 3.1 目标定位(Object localization)
图片分类:算法遍历图片，判断其中的对象是不是汽车。

定位分类:不仅要用算法判断图片中是不是一辆汽车，还要在图片中标记出它的位置，用边框或红色方框把汽车圈起来。

![object_localization_1.png](https://i.imgur.com/hXiijNa.png)

目标标签y的定义：y=[p<sub>c</sub> b<sub>x</sub> b<sub>y</sub> b<sub>h</sub> b<sub>w</sub> c<sub>1</sub> c<sub>2</sub> c<sub>3</sub>]<sup>T</sup>
p<sub>c</sub>表示是否含有对象，其中p<sub>c</sub>=1表示含有对象；
如果检测到对象，就输出被检测对象的边界框参数b<sub>x</sub> b<sub>y</sub> b<sub>h</sub> b<sub>w</sub>；
c<sub>1</sub> c<sub>2</sub> c<sub>3</sub>表示该对象属于1-3类中的哪一类；

损失函数：

![object_localization_loss_2.png](https://i.imgur.com/YJoVlDK.png)

如果图片中存在定位对象，那么y<sub>1</sub>=1，所以y<sub>1</sub>=p<sub>c</sub>，同样地，如果图片中存在定位对象，p<sub>c</sub>=1，损失值就是不同元素的平方和。

另一种情况是y<sub>1</sub>=0，也就是p<sub>c</sub>=0，损失值是(y<sub>1</sub>hat-y<sub>1</sub>)<sup>2</sup>，因为对于这种情况，不用考虑其它元素，只需要关注神经网络输出p<sub>c</sub>的准确度。

实际上，通常做法是对边界框坐标应用平方差或类似方法，对p<sub>c</sub>应用逻辑回归函数。

### 3.2 目标定位(Landmark detection)
神经网络可以通过输出图片上特征点的(x,y)坐标来实现对目标特征的识别。

![landmark_detection_1.png](https://i.imgur.com/LhNxBwV.png)

总体思路：选定特征点个数，并生成包含这些特征点的标签训练集，然后利用神经网络输出脸部关键特征点的位置。

具体做法是，准备一个卷积网络和一些特征集，将人脸图片输入卷积网络，输出1或0，1表示有人脸，0表示没有人脸，然后输出(l<sub>1x</sub>,l<sub>1y</sub>)..直到(l<sub>64x</sub>,l<sub>64y</sub>)，l代表一个特征，这里有129个输出单元，其中1表示图片中有人脸，因为有64个特征，64×2=128，所以最终输出128+1=129个单元
，由此实现对图片的人脸检测和定位。还需要准备一个标签训练集，也就是图片x和标签y的集合，这些点都是人为辛苦标注的。

### 3.3 目标检测(Object detection)
![object_detection_1.png](https://i.imgur.com/F381aeO.png)

滑动窗口目标检测算法:思路是以固定步幅移动窗口，遍历图像的每个区域，把这些剪切后的小图像输入卷积网络，对每个位置按0或1进行分类。

训练完这个卷积网络，就可以用它来实现滑动窗口目标检测，具体步骤如下：

首先选定一个特定大小的窗口，从上到下，从左到右滑动红色方框依次遍历整张图像，依次重复操作，直到这个窗口滑过图像的每一个角落；

![object_detection_2.png](https://i.imgur.com/TJgZKsU.png)

然后，选择一个更大的窗口重复以上步骤；

![object_detection_3.png](https://i.imgur.com/MUcCoZf.png)

然后，再次选择一个更大的窗口重复以上步骤；
这样做，不论汽车在图片的什么位置，总有一个窗口可以检测到它。

滑动窗口目标检测算法缺点：就是计算成本，因为在图片中剪切出太多小方块，卷积网络要一个个地处理。如果选用的步幅很大，显然会减少输入卷积网络的窗口个数，但是粗糙间隔尺寸可能会影响性能。反之，如果采用小粒度或小步幅，传递给卷积网络的小窗口会特别多，这意味着超高的计算成本。

### 3.4 卷积的滑动窗口实现(Convolutional implementation of sliding windowns)
#### 3.4.1 用卷积层代替全连接层
![convolutional_sliding_windown_1.png](https://i.imgur.com/UXTOKk4.png)

上图中，第一行是含有全连接层的卷积神经网络。

第二行中，使用400个5x5x16的过滤器进行卷积得到400个1x1的输出代替全连接层，然后再用400个1x1的过滤器卷积，得到下一层1x1x400的卷积层，最后使用softmax得到4个分类出现的概率。

#### 3.4.2 卷积的滑动窗口实现
![convolutional_sliding_windown_2.png](https://i.imgur.com/5aFrPFs.png)

最终，在输出层这4个子方块中，蓝色的是图像左上部分14×14的输出，右上角方块是图像右上部分的对应输出，左下角方块是输入层左下角，也就是这个14×14区域经过卷积网络处理后的结果，同样，右下角这个方块是卷积网络处理输入层右下角14×14区域的结果。

**卷积操作的原理**是不需要把输入图像分割成四个子集，分别执行前向传播，而是把它们作为一张图片输入给卷积网络进行计算，其中的公共区域可以共享很多计算。

![convolutional_sliding_windown_3.png](https://i.imgur.com/BIlewzn.png)

在卷积层上应用滑动窗口算法它提高了整个算法的效率。不过这种算法仍然存在一个缺点，就是边界框的位置可能不够准确，甚至边界框不是方形。

### 3.5 Bounding Box预测(Bounding box predictions)
YOLO算法：基本思路是使用图像分类和定位算法，然后将算法应用到9个格子上。

![bounding_box_1.png](https://i.imgur.com/YhLXeTl.png)

YOLO算法做的就是，取两个对象的中点，然后将这个对象分配给包含对象中点的格子。所以左边的汽车就分配到这个格子上（编号4），然后这辆Condor（车型：神鹰）中点在这里，分配给这个格子（编号6）。所以即使中心格子（编号5）同时有两辆车的一部分，我们就假装中心格子没有任何我们感兴趣的对象，所以对于中心格子，分类标签y和这个向量类似，和这个没有对象的向量类似，即y=[0 ? ? ? ? ? ? ?]<sup>T</sup>，其它的网格于此类似。对于这里9个格子中任何一个，都会得到一个8维输出向量，因为这里是3×3的网格，所以有9个格子，总的输出尺寸是3×3×8，所以目标输出是3×3×8。

![bounding_box_2.png](https://i.imgur.com/X8BAG1O.png)

YOLO算法的优点:
1. 在于神经网络可以输出精确的边界框，所以测试的时候，输入图像x，然后跑正向传播，直到得到这个输出y。
2. 这是一个卷积实现，实际上它的运行速度非常快，可以达到实时识别。

使用更精细的网格，多个对象分配到同一个格子得概率就小得多。所以要注意，
- 首先这和图像分类和定位算法非常像，就是它显式地输出边界框坐标，所以这能让神经网络输出边界框，可以具有任意宽高比，并且能输出更精确的坐标，不会受到滑动窗口分类器的步长大小限制。
- 其次，这是一个卷积实现，并没有在3×3网格上跑9次算法，或者，如果用的是19×19的网格，19平方是361次，所以不需要让同一个算法跑361次。相反，这是单次卷积实现，但使用了一个卷积网络，有很多共享计算步骤，在处理这3×3计算中很多计算步骤是共享的，或在19×19的网格，所以这个算法效率很高。

### 3.6 交叉比(intersection over union)
并交比函数可以用来评价对象检测算法。

![iou_1.png](https://i.imgur.com/PVvjlBG.png)

more generally,iou is a measure of the overlap between two bounding boxes.

交并比（loU）函数做的是计算两个边界框交集和并集之比。两个边界框的并集是这个区域，就是属于包含两个边界框区域（绿色阴影表示区域），而交集就是这个比较小的区域（橙色阴影表示区域），那么交并比就是交集的大小，这个橙色阴影面积，然后除以绿色阴影的并集面积。

一般约定，0.5是阈值，用来判断预测的边界框是否正确。一般是这么约定，但如果希望更严格一点，可以将loU定得更高，比如说大于0.6或者更大的数字，但loU越高，边界框越精确。

### 3.7 非极大值抑制(Non-max suppression)
到目前为止学到的对象检测中的一个问题是，算法可能对同一个对象做出多次检测，所以算法不是对某个对象检测出一次，而是检测出多次。非极大值抑制这个方法可以确保算法对每个对象只检测一次。

![non-max_suppression_1.png](https://i.imgur.com/KVJl7Uj.png)

实践中当运行对象分类和定位算法时，对于每个格子都运行一次，所以这个格子（编号1）可能会认为这辆车中点应该在格子内部，这几个格子（编号2、3）也会这么认为。对于左边的车子也一样，所以不仅仅是这个格子，如果这是以前见过的图像，不仅这个格（编号4）子会认为它里面有车，也许这个格子（编号5）和这个格子（编号6）也会，也许其他格子也会这么认为，觉得它们格子内有车。非极大值抑制做的就是清理这些检测结果。这样一辆车只检测一次，而不是每辆车都触发多次检测。

![non-max_suppression_2.png](https://i.imgur.com/vq9EAxh.png)

去掉所有概率p<sub>c</sub>小于或等于某个阈值的边界框
while remaining boxes:
1.选择概率p<sub>c</sub>最高的边界框，然后把它输出成预测结果；
2.接下来把这些和输出边界框有高重叠面积和上一步输出边界框有很高交并比的边界框全部抛弃;

### 3.8 Anchor Boxes
上述算法只适用于单目标检测，也就是每个网格只能检测一个对象。要将该算法运用在多目标检测上，需要用到Anchor Boxes。

![anchor_1.png](https://i.imgur.com/6dKDiFO.png)

定义新的类别标签y=[p<sub>1</sub> b<sub>x1</sub> b<sub>y1</sub> b<sub>h1</sub> b<sub>w1</sub> c<sub>1</sub> c<sub>2</sub> c<sub>3</sub> p<sub>2</sub> b<sub>x2</sub> b<sub>y2</sub> b<sub>h2</sub> b<sub>w2</sub> c<sub>1</sub> c<sub>2</sub> c<sub>3</sub>]<sup>T</sup> 前8个参数和anchor box 1，后8个参数和anchor box 2。

#### Anchor box algorithm
![anchor_algorithm_2.png](https://i.imgur.com/zx8iMGM.png)

用anchor box之前，对于训练集图像中的每个对象，都根据对象中点位置分配到对应的格子中，所以输出y就是3×3×8，因为是3×3网格，对于每个网格位置，输出向量，包含p<sub>c</sub>，然后边界框参数b<sub>x</sub> b<sub>y</sub> b<sub>h</sub> b<sub>w</sub>，然后c<sub>1</sub> c<sub>2</sub> c<sub>3</sub>。

![anchor_algorithm_3.png](https://i.imgur.com/Zux0oPM.png)

使用anchorbox，现在每个对象都和之前一样分配到同一个格子中，分配到**1.对象中点所在的格子中**，以及分配到和对象形状交并比最高的anchor box中。所以这里有两个anchor box，如果对象形状是红色框，anchor box 1形状是纵向紫色框，anchor box 2形状是横向紫色框，然后观察哪一个anchor box和实际边界的交并比更高，不管选的是哪一个，这个对象不只分配到一个格子，而是**2.分配到一对anchor box**，即(grid cell，anchor box)对，这就是对象在目标标签中的编码方式。

建立anchor box是为了处理两个对象出现在同一个格子的情况，实践中这种情况很少发生，特别是如果用的是19×19网格而不是3×3的网格，两个对象中点处于361个格子中同一个格子的概率很低，确实会出现，但出现频率不高。

### 3.9 YOLO算法(Putting it together:YOLO algorithm)
把所有组件组装在一起构成YOLO对象检测算法。

![yolo_1.png](https://i.imgur.com/L4Wpqc4.png)

上图中，输出标签是3x3x2x8或者(19x19x2x8)，输入训练集是100x100x3。

Outputting the non-max supressed outputs

![yolo_2.png](https://i.imgur.com/KPsAwwp.png)

如果使用两个anchor box，那么对于9个格子中任何一个都会有两个预测的边界框，其中一个的概率很低。但9个格子中，每个都有**两个预测的边界框**，接下来**抛弃概率很低的预测**。

如果希望检测行人，汽车和摩托车，那么要做的是，**对于每个类别单独运行非极大值抑制**，处理预测结果所属类别的边界框，用非极大值抑制来处理行人类别，用非极大值抑制处理车子类别，然后对摩托车类别进行非极大值抑制，运行三次来得到最终的预测结果。所以算法的输出最好能够检测出图像里所有的车子，还有所有的行人。

### 3.10 候选区域(Region proposals)
R-CNN，带区域的卷积网络；算法尝试选出一些区域，在这些区域上运行卷积网络分类器是有意义的。

选出候选区域的方法是运行图像分割算法，分割的结果是下边的图像，为了找出可能存在对象的区域。比如说，分割算法在这里得到一个色块，所以可能会选择这样的边界框（编号1），然后在这个色块上运行分类器，就像这个绿色的东西（编号2），在这里找到一个色块，接下来还会在那个矩形上（编号2）运行一次分类器，看看有没有东西。在这种情况下，如果在蓝色色块上（编号3）运行分类器，希望能检测出一个行人，如果在青色色块(编号4)上运行算法，也许可以发现一辆车。

![r-cnn_1.png](https://i.imgur.com/mo9dBJ6.png)

所以这个细节就是所谓的分割算法，先找出可能2000多个色块，然后在这2000个色块上放置边界框，然后在这2000个色块上运行分类器，这样需要处理的位置可能要少的多，可以减少卷积网络分类器运行时间，比在图像所有位置运行一遍分类器要快。特别是这种情况，现在不仅是在方形区域（编号5）中运行卷积网络，还会在高高瘦瘦（编号6）的区域运行，尝试检测出行人，然后在很宽很胖的区域（编号7）运行，尝试检测出车辆，同时在各种尺度运行分类器。

#### Faster algorithms
R-CNN缺点是选择候选区域的聚类步骤仍然非常缓慢。

![r-cnn_2.png](https://i.imgur.com/xEC1A3z.png)