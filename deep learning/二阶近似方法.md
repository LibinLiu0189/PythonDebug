### 1.0 Hessian矩阵
设n多元实函数f(x<sub>1</sub>,x<sub>2</sub>,...,x<sub>n</sub>)在点M<sub>0</sub>(a<sub>1</sub>,a<sub>2</sub>,...,a<sub>n</sub>)的邻域内有二阶连续偏导，若有：

![hissian_1.jpg](https://i.imgur.com/62DULXQ.png)

并且

![hissian_2.jpg](https://i.imgur.com/iuvDAHp.png)

则有如下结果：</br>
(1)当A正定矩阵时，f(x<sub>1</sub>,x<sub>2</sub>,...,x<sub>n</sub>)在M<sub>0</sub>(a<sub>1</sub>,a<sub>2</sub>,...,a<sub>n</sub>)处是极小值；</br>
(2)当A负定矩阵时，f(x<sub>1</sub>,x<sub>2</sub>,...,x<sub>n</sub>)在M<sub>0</sub>(a<sub>1</sub>,a<sub>2</sub>,...,a<sub>n</sub>)处是极大值；</br>
(3)当A不定矩阵时，M<sub>0</sub>(a<sub>1</sub>,a<sub>2</sub>,...,a<sub>n</sub>)不是极值点。</br>
(4)当A为半正定矩阵或半负定矩阵时，M<sub>0</sub>(a<sub>1</sub>,a<sub>2</sub>,...,a<sub>n</sub>)是“可疑”极值点，尚需要利用其他方法来判定。

实例：

![hissian_3.png](https://i.imgur.com/Sg2WiBs.png)

### 2.0 牛顿法
牛顿法是基于二阶泰勒级数展开在某点 θ<sub>0</sub> 附近来近似 J(θ) 的优化方法，其忽略了高阶导数：

![niudun_1.png](https://i.imgur.com/swCBRZq.png)

其中，H是J相对于θ的Hessian矩阵在θ<sub>0</sub>处的估计。如果再求解函数的临界点，将得到牛顿参数更新规则：

![niudun_2.png](https://i.imgur.com/nSPil7B.png)

因此，对于局部的二次函数(具有正定的H)，用H<sup>-1</sup>重新调整梯度，牛顿法会直接跳到极小值。如果目标函数是凸的但非二次的(有高阶项)，该更新将是迭代的，得到和牛顿法相关的算法：

![niudun_3.png](https://i.imgur.com/CAkDaVW.png)

上述讨论了牛顿法只适用于Hessian矩阵式正定的情况。在深度学习中，目标函数的表面通常非凸。如果Hessian矩阵的特征值并不都是正的，牛顿法实际上会导致更新朝错误的方向移动。这种情况可以通过正则化Hessian矩阵来避免。常用的正则化策略包括在Hessian矩阵对角线上增加常数α。正则化更新变为：

![niudun_4.png](https://i.imgur.com/NTBinIp.png)

### 3.0 共轭梯度

### Q&A
#### 为什么牛顿法比梯度下降法快?
牛顿法是二阶收敛，梯度下降是一阶收敛，所以牛顿法就更快。如果更通俗地说的话，比如你想找一条最短的路径走到一个盆地的最底部，梯度下降法每次只从你当前所处位置选一个坡度最大的方向走一步，牛顿法在选择方向时，不仅会考虑坡度是否够大，还会考虑你走了一步之后，坡度是否会变得更大。所以，可以说牛顿法比梯度下降法看得更远一点，能更快地走到最底部。

但是牛顿法要计算Hessian矩阵，比较费时间。

参考:

1. [最优化问题中，牛顿法为什么比梯度下降法求解需要的迭代次数更少？](https://www.zhihu.com/question/19723347)