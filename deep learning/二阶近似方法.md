### Hessian矩阵
设n多元实函数f(x<sub>1</sub>,x<sub>2</sub>,...,x<sub>n</sub>)在点M<sub>0</sub>(a<sub>1</sub>,a<sub>2</sub>,...,a<sub>n</sub>)的邻域内有二阶连续偏导，若有：

![hissian_1.jpg](https://i.imgur.com/62DULXQ.png)

并且

![hissian_2.jpg](https://i.imgur.com/iuvDAHp.png)

则有如下结果：</br>
(1)当A正定矩阵时，f(x<sub>1</sub>,x<sub>2</sub>,...,x<sub>n</sub>)在M<sub>0</sub>(a<sub>1</sub>,a<sub>2</sub>,...,a<sub>n</sub>)处是极小值；</br>
(2)当A负定矩阵时，f(x<sub>1</sub>,x<sub>2</sub>,...,x<sub>n</sub>)在M<sub>0</sub>(a<sub>1</sub>,a<sub>2</sub>,...,a<sub>n</sub>)处是极大值；</br>
(3)当A不定矩阵时，M<sub>0</sub>(a<sub>1</sub>,a<sub>2</sub>,...,a<sub>n</sub>)不是极值点。</br>
(4)当A为半正定矩阵或半负定矩阵时，M<sub>0</sub>(a<sub>1</sub>,a<sub>2</sub>,...,a<sub>n</sub>)是“可疑”极值点，尚需要利用其他方法来判定。

实例：

![hissian_3.png](https://i.imgur.com/Sg2WiBs.png)

### 牛顿法
牛顿法是基于二阶泰勒级数展开在某点 θ<sub>0</sub> 附近来近似 J(θ) 的优化方法，其忽略了高阶导数：

![niudun_1.png](https://i.imgur.com/swCBRZq.png)

其中，H是J相对于θ的Hessian矩阵在θ<sub>0</sub>处的估计。如果再求解函数的临界点，将得到牛顿参数更新规则：

![niudun_2.png](https://i.imgur.com/nSPil7B.png)

因此，对于局部的二次函数(具有正定的H)，用H<sup>-1</sup>重新调整梯度，牛顿法会直接跳到极小值。如果目标函数是凸的但非二次的(有高阶项)，该更新将是迭代的，得到和牛顿法相关的算法：

![niudun_3.png](https://i.imgur.com/CAkDaVW.png)

上述讨论了牛顿法只适用于Hessian矩阵式正定的情况。在深度学习中，目标函数的表面通常非凸。如果Hessian矩阵的特征值并不都是正的，牛顿法实际上会导致更新朝错误的方向移动。这种情况可以通过正则化Hessian矩阵来避免。常用的正则化策略包括在Hessian矩阵对角线上增加常数α。正则化更新变为：

![niudun_4.png](https://i.imgur.com/NTBinIp.png)

### 共轭梯度

