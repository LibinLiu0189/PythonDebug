### 导论
统计机器学习中有一个经典的假设:Source Domain 和 Target Domain的数据分布是一致的。也就是说，训练数据和测试数据是满足相同分布的。这是通过训练数据获得的模型能够在测试集上获得好的效果的一个基本保障。

Convariate Shift是指训练集的样本数据和目标样本集分布不一致时，训练得到的模型无法很好的Generalization。它是分布不一致假设之下的一个分支问题，也就是指Sorce Domain和Target Domain的条件概率一致的，但是其边缘概率不同。的确，对于神经网络的各层输出，在经过了层内操作后，各层输出分布就会与对应的输入信号分布不同，而且差异会随着网络深度增大而加大了，但每一层所指向的Label仍然是不变的。

解决办法：一般是根据训练样本和目标样本的比例对训练样本做一个矫正。所以，通过引入Bactch Normalization来标准化某些层或者所有层的输入，从而固定每层输入信息的均值和方差。

### 1.0 Batch Normalization
1. 把具有不同尺度的特征映射到同一个坐标系，具有相同的尺度(相似特征分布)，使激活函数分布在线性区间，结果就是加大了梯度，让模型更大胆的进行梯度下降。
2. 一定程度上消除了噪声、质量不佳等各种原因对模型权值更新的影响。
3. 破坏原来的数据分布，一定程度上缓解了过拟合。
4. 更容易跳出局部最小值。

含有batch-norm的神经网络计算步骤：

![mini-batch-norm_1.png](https://i.imgur.com/wFGzFb4.png)

对于含有m个节点的某一层神经网络，对z进行操作的步骤为:

![mini-batch-norm_2.png](https://i.imgur.com/sfNipn1.png)

第一，这里的常规归一化实际上就是改变了一个mini-batch中样本的分本，由原来的某个分布转化成均值为0方差为1的标准分布；</br>
第二，仅转化了分布还不行，因为转化过后可能改变了输入的取值范围，因此需要赋予一定的放缩和平移能力，即将归一化后的输入通过一个仿射变换的子网络。其中的`γ`、`β`并不是超参数，而是两个需要学习的参数，神经网络自己去学着使用和修改这两个扩展参数。这样神经网络就能自己慢慢琢磨出前面的标准化操作到底有没有起到优化的作用。如果没有起到作用，就使用`γ`、`β`来抵消一些之前进行过的标准化的操作。</br>
第三，这里的所有操作都是可微分的，也就使得了梯度后向传播算法在这里变得可行；

### Batch Normalization back propagation
![batch normalization bp.jpg](https://i.imgur.com/iVeq17j.jpg)

### 2.0 Batch Norm为什么会奏效？
通过归一化所有的输入特征值x，以获得类似范围的值，可以加快学习。`Batch-norm`是类似的道理

当前的获得的经验无法适应新样本、新环境时，便会发生“`Covariate Shift`”现象。 对于一个神经网络，前面权重值的不断变化就会带来后面权重值的不断变化，批标准化减缓了隐藏层权重分布变化的程度。采用批标准化之后，尽管每一层的z还是在不断变化，但是它们的**均值和方差将基本保持不变，限制了在前层的参数更新会影响数值分布的程度，使得后面的数据及数据分布更加稳定，减少了前面层与后面层的耦合**，使得每一层不过多依赖前面的网络层，最终加快整个神经网络的训练。

### 2.1 When to use BN?
1. 在神经网络训练时遇到收敛速度很慢，或梯度爆炸等无法训练的状况时可以尝试BN来解决。
2. 另外，在一般使用情况下也可以加入BN来加快训练速度，提高模型精度。

参考：
1. [Batch Normalization原理及其TensorFlow实现](https://www.cnblogs.com/bonelee/p/8528722.html)
2. [深度学习中 Batch Normalization为什么效果好？](https://www.zhihu.com/question/38102762)