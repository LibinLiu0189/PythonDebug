### 过拟合
![过拟合](https://pic4.zhimg.com/v2-62b74afd353d7fdaf9c8d6b20d38d3e1_r.jpg)

针对过拟合问题，通常会考虑两种途径来解决：
a) 减少特征的数量：
- 人工的选择保留哪些特征；
- 模型选择算法（之后的课程会介绍）

b) 正则化
- 保留所有的特征，但是降低参数的量/值；
- 正则化的好处是当特征很多时，每一个特征都会对预测y贡献一份合适的力量；

### 5.0 正则化
该算法可以平衡特征的数量、精确度、泛化能力，牺牲一定的准确性，而增加一定的泛化性能，避免完美拟合。

```
from sklearn import linear_model
clf = linear_model.Lasso()
clf.fit(features,labels)
print(clf.coef_)            #输出每个特征所占的比例，为0时说明被舍弃了
```

#### L1正则化和L2正则化的说明

- **L1正则化**是指权值向量w中各个元素的绝对值之和，通常表示为||w||1
- **L2正则化**是指权值向量w中各个元素的平方和然后再求平方根（可以看到Ridge回归的L2正则化项有平方符号），通常表示为||w||2

贝叶斯学派的观点：正则化其实就是对模型的参数设定一个先验。
L1正则是laplace先验，l2是高斯先验，分别由参数sigma确定。

#### 下面是L1正则化和L2正则化的作用

- L1正则化可以产生**稀疏**权值矩阵，即产生一个稀疏模型，可以用于特征选择(删掉一些冗余的特征)
- L2正则化可以防止模型过拟合(`overfitting`)；一定程度上，L1也可以防止过拟合

### 5.1 Lasso回归的损失函数
下图是Python中Lasso回归的损失函数，式中加号后面一项α||w||1即为L1正则化项。

![Lasso.png](https://i.imgur.com/mnBUkTZ.png)

下图是Python中Ridge回归的损失函数，式中加号后面一项![L2Reg](https://i.imgur.com/PhdhMdR.png)即为L2正则化项。

![Ridge.png](https://i.imgur.com/IvfSwSP.png)


#### 5.2.1 L1正则化和特征选择
假设有如下带L1正则化的损失函数： 

![L1_reg_func_1](https://i.imgur.com/DPdbKJv.png)

其中J0是原始的损失函数，加号后面的一项是L1正则化项，α是正则化系数。注意到L1正则化是权值的绝对值之和，J是带有绝对值符号的函数，因此J是不完全可微的。机器学习的任务就是要通过一些方法（比如梯度下降）求出损失函数的最小值。当我们在原始损失函数J0后添加L1正则化项时，相当于对J0做了一个约束。令`L=α∑w|w|`，则`J=J0+L`，此时我们的任务变成在L约束下求出J0取最小值的解。考虑二维的情况，即只有两个权值w1和w2，此时`L=|w1|+|w2|`对于梯度下降法，求解J0的过程可以画出等值线，同时L1正则化的函数L也可以在w1w2的二维平面上画出来。如下图：

![L1Reg_model_1.png](https://i.imgur.com/sHE39w4.png)

图中等值线是J0的等值线，黑色方形是L函数的图形。在图中，当J0等值线与L图形首次相交的地方就是最优解。上图中J0与L在L的一个顶点处相交，这个顶点就是最优解。注意到这个顶点的值是(w1,w2)=(0,w)。可以直观想象，因为L函数有很多『突出的角』（二维情况下四个，多维情况下更多），J0与这些角接触的机率会远大于与L其它部位接触的机率，而在这些角上，会有很多权值等于0，这就是为什么L1正则化可以产生稀疏模型，进而可以用于特征选择。

而正则化前面的系数α，可以控制L图形的大小。α越小，L的图形越大（上图中的黑色方框）；α越大，L的图形就越小，可以小到黑色方框只超出原点范围一点点，这是最优点的值(w1,w2)=(0,w)中的w可以取到很小的值。

类似，假设有如下带L2正则化的损失函数： 

![L1_reg_func_2](https://i.imgur.com/SPrjJvh.png)

同样可以画出他们在二维平面上的图形，如下：

![L2Reg_model_2.png](https://i.imgur.com/LFVWAkg.png)

二维平面下L2正则化的函数图形是个圆，与方形相比，被磨去了棱角。因此J0与L相交时使得w1或w2等于零的机率小了许多，这就是为什么L2正则化不具有稀疏性的原因。

#### 5.2.2 L2正则化和过拟合

![L2_reg_model_2](https://i.imgur.com/RyXIsIW.png)

其中λ就是正则化参数。从上式可以看到，与未添加L2正则化的迭代公式相比，每一次迭代，θj都要先乘以一个小于1的因子，从而使得θj不断减小，因此总得来看，θ是不断减小的。

λ越大，θj衰减得越快

[机器学习中正则化项L1和L2的直观理解](https://blog.csdn.net/jinping_shi/article/details/52433975)

#### L2正则化
![L2正则化](http://52opencourse.com/?qa=blob&qa_blobid=14870802024620705686)

对参数添加一个系数，使施加的影响最小。

