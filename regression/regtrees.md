### 1.0 ID3

#### 1.1.0 信息增益的算法

输入：训练数据集D和特征A;
输出：计算特征A对训练数据集D的信息增益g(D，A)

(1) 计算数据集D的经验熵H(D)

![tree_1.png](https://i.imgur.com/hFQ1rsp.png)

(2) 计算特征A对数据集D的经验条件熵H(D|A)

![tree_2.png](https://i.imgur.com/0vY5lwT.png)

(3) 计算信息增益

![tree_3.png](https://i.imgur.com/afB7oam.png)



#### 1.1.1 ID3原理
原理：每次选取当前最佳的特征来分割数据，并按照该特征的所有可能取值来切分。

### 1.2 ID3的缺陷
#### 1.2.1 ID3 缺陷:切分方式过于迅速
也就是说，如果一个特征有 4 种取值，那么数据将被切分成 4 份。一旦按照某特征切分后，该特征在之后的算法执行过程中将不会再起作用，所以有观点认为这种**切分方式过于迅速**。

#### 1.2.2 ID3 缺陷: 不能直接处理连续型特征
只有事先将连续型特征转换成离散型，才能在 ID3 算法中使用。但这种转换过程会破坏连续型变量的内在性质。

举例：

如果我们在上面的数据记录中加一个姓名属性，假设14条记录中的每个人姓名不同，那么信息增益就会选择姓名作为最佳属性，因为按姓名分裂后，每个组只包含一条记录，而每个记录只属于一类（要么购买电脑要么不购买，信息量计算为1/14(-1/1log1/1-0/1log0/1)*14=0，信息增益最大），因此纯度最高，以姓名作为测试分裂的结点下面有14个分支。但这样的分类没有意义，它**没有任何泛化能力**。

#### 1.2.3 二元切分法
另外一种方法是二元切分法，即每次把数据集切分成两份。如果数据的某特征值等于切分所要求的值，那么这些数据就进入树的左子树，反之则进入树的右子树。另外，二元切分法也节省了树的构建时间，但这点意义也不是特别大，因为这些树构建一般是离线完成，时间并非需要重点关注的因素。

### 1.2  信息增益率
信息增益值的大小是相对于训练数据集而言的，并没有绝对意义。在分类问题困难时，也就是说在训练数据集的经验熵大的时候，信息增益值会偏大。反之，信息增益值偏小。当经验熵很大时，训练出来的形状是一棵庞大且深度很浅的树，这样的划分是极为不合理的。
使用信息增益比可以对这一问题进行校验。

![tree_4.png](https://i.imgur.com/Dfdg1Ct.png)


### 2.0 划分分支方式
- ID3 是信息增益分支
- C4.5 是信息增益率分支
- CART 做分类工作时，采用 GINI 值作为节点分裂的依据；回归时，采用样本的最小方差作为节点的分裂依据。

**工程上总的来说**:
CART 和 C4.5 之间主要差异在于分类结果上，CART 可以回归分析也可以分类，C4.5 只能做分类；C4.5 子节点是可以多分的，而 CART 是无数个二叉子节点；

参考：

[为什么说c45克服了ID3偏向选择取值较多的属性的这一问题？](https://www.zhihu.com/question/27109632 "为什么说c45克服了ID3偏向选择取值较多的属性的这一问题？")

[C45为什么使用信息增益比来选择特征？](https://www.zhihu.com/question/22928442 "C45为什么使用信息增益比来选择特征？")