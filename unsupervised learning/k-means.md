## K-means
### 1.0 K-means
它可以发现 K 个不同的簇, 且每个簇的中心采用簇中所含值的均值计算而成.簇个数 K 是用户指定的, 每一个簇通过其质心（centroid）, 即簇中所有点的中心来描述.

- 优点: 容易实现
- 缺点:可能收敛到局部最小值, 在大规模数据集上收敛较慢

使用数据类型 : 数值型数据

#### 1.1  K-means的局限
1. Kmeans是爬山算法,它非常**依赖于你的初始聚类中心所处的位置**，所以同一个训练集训练出的模型，可能预测出不一样的结果；

2. 局部最小值(理由同上)

![kmeans局部最小](https://i.imgur.com/wvXiaBV.png)

3. 差的局部最小值

![kmeans差的局部最小值](https://i.imgur.com/CIZnV0A.png)


### 2.0 K-Means伪代码实现
```
1. 创建 k 个点作为起始质心（通常是随机选择）
2. 当任意一个点的簇分配结果发生改变时
    2.1 对数据集中的每个数据点
        2.1.1 对每个质心
        2.1.2 计算质心与数据点之间的距离
        2.1.3 将数据点分配到距其最近的簇
    2.2 对每一个簇, 计算簇中所有点的均值并将均值作为质心
```

### 3.0 二分 K-Means 聚类算法
背景：因为K-means可能偶尔会陷入局部最小值

#### 3.1 二分K-Means伪代码实现
```
1. 将所有点看成一个簇
2. 当簇数目小雨 k 时
3. 对于每一个簇
    3.1 计算总误差
    3.2 在给定的簇上面进行 KMeans 聚类（k=2）
    3.3 计算将该簇一分为二之后的总误差
4. 选择使得误差最小的那个簇进行划分操作
```

## 更多聚类
----------

### 4.0 单连锁聚类


### 5.0 特征缩放
#### 5.1.1 调节比例
![调节比例](https://i.imgur.com/GfRSDjV.png)

- 优点：预估输出相对稳定
- 缺点：如果输出特征中有异常值，那么特征缩放就会比较棘手（最大值最小值可能是极端值）

###### 应用
```
from sklearn.preprocessing import MinMaxScaler
import numpy
#这里numpy数组中的是特征，因为此处特征只有一个，所以看起来是这样的
#因为这里应该作为一个浮点数进行运算，所以数字后面要加.
weights = numpy.array([[115.],[140.],[175.]])
scaler = MinMaxScaler()
rescaled_weight = scaler.fit_transform(weights)
print rescaled_weight
```

#### 5.1.1 标准化
![标准化](https://i.imgur.com/9IDUEHR.png)
特征标准化使每个特征的值有平均值(zero-mean)和单位方差(unit-variance)。

#### 5.2.1 哪些机器学习算法会受到特征缩放的影响？
- SVM(rbf)计算最大距离时就是这种情况。如果我们把某一点增大至其他点的两倍，那么它的数值也会扩大一倍
- K-均值聚类也是。计算各数据点到集群中心的距离

