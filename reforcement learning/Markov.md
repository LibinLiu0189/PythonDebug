## 强化学习概念
通过对不同的结果奖励和惩罚来增强强化学习的进程。

## 与机器学习中概念进相比
- 监督学习：给定多组(x,y)(x,y),拟合出一个Loss 最低的 f(x)f(x)
- 无监督学习:给定多组 xx,找到一组函数集可以联合描述 xx的变化特性。
- 强化学习：与监督学习相似，给定多组(x,y)(x,y),同时一个抉择向量 zz。强化学习可以称之为监督学习的扩展，扩展了一套决策方案而并非单一的 GD

### 增强学习
![reinforcement learning_1.png](https://i.imgur.com/DeUdbI4.png)

- states:决策时可能经历的状态集
- model:正在进行的博弈的原则
p(s'|s,a):在状态s中采取动作A后最终落入s'的概率
- actions:所有被允许的决策动作
- reward:通过过程或者最终结果对当前执行行为进行评定。
R(s),R(s,a),R(s,a,s')
- policy:在一个特定的状态中该采取什么动作

### MDPtipS
- MDP的关键是奖励函数的设置，最终规则集合包含了一系列奖惩措施
- MDP框架的核心是让程序关注我在哪里，该做什么，是否会得到的奖励

### reward
- 奖励是让学习算法衡量决策好坏的标准
- 当我们想让获得的决策尽量步数少时，可以将默认奖励值设置为一个不大的负数单位：走过步数的平均要小于且与终止``吸收态有一定的数值距离


### 影响奖励的几大因素

- 默认奖励数值
- 吸收态奖励数值
- 剩余的时间步
- 每一步的偏差发生率（不按规定执行的概率）

### 奖励区别
![奖励区别](http://ovsouy67p.bkt.clouddn.com/15094407439418.jpg)

- 这是一个关于默认奖励不同的情况的两个例子，上面的区块默认奖励为+2，下面的默认奖励为-2
- 对于上面的而言，奖励为正值。为了能获得到更多的奖励，我们不能让程序进入停止游戏区间，最好的办法就是撞墙（不断的停留原地所以获得奖励）
- 对于下面的区间，由于奖励为负值，我们需要尽快的离开游戏。右下角的方向为上的原因是，如果当前为其他方向，那么肯定会有至少一个-2出现在奖励序列里。所以最好的方法是：直接终止，取得那个-1的红色区间
- 通过这个例子可以看出，当奖励函数不同，强化学习最后得到的规则集合也是截然不同的。

### 偏向稳定性
如果有两个时间序列 A:s0,s1,s2 与 B:s0,s′1,s′2，如果 A>B 则 AB 的去掉相同元素的子序列仍然满足s1,s2>s′1,s′2s1,s2>s1′,s2′。我们称这种现象为偏向稳定性

![偏向稳定性](https://i.imgur.com/R2zmNTZ.png)

### 偏向稳定性与奖励序列
强化学习中，奖励是一个序列性的问题，也就是状态序列。学习的目的是希望最后能得到的总奖励最高。但请一定要注意时序的长短问题,即**时间长度是否无限**。 

![奖励序列_1](https://i.imgur.com/px5YLZK.png)

看上面的例子，如果问当上面情况一直重复时，哪个会更好？

答案是：两者都一样，因为有部分+2出现。但当时间无限时，上面得到的奖励: 

![奖励序列_2](https://i.imgur.com/mDIYzR3.png)

上下相等

- 显然在无限时间内,讨论不受控制的没有负奖励的两个奖励序列的区别是无意义的；
- 但是如上文单独讨论的一个序列的奖励大小也是无意义的。因为无穷大不是一个数。
- 这时我们就需要对无限增长的正奖励一个界定空间，即**折扣期望**

### 折扣期望
![奖励序列_3.png](https://i.imgur.com/AQc7BQk.png)

状态序列的效用是所有奖励的总和。

![奖励序列_4.png](https://i.imgur.com/yiCxX60.png)

此时当 t 达到一定数量时，上式的值会停留在一个Rmax数值上,这里上式实际等价于

![等比数列_1.png](https://i.imgur.com/jzhrS0C.png)

意义：
- 让获得的奖励具有边界。这里实际是将RmaxRmax值直接类比为无穷大 
- 折扣期望级数加和序列退化为等比序列
- 等比序列又允许我们使用无穷多的数字跟在奖励值后做乘积，但结果的是一个有穷的数字
- 允许我们在有穷的时间接触到无穷大
- 使之前的无穷时域具有了有穷性

### 假设
### 策略
最优决策:

![最优决策_1.png](https://i.imgur.com/SoBPFBx.png)

E[]为整体期望，表示希望找到一种可以**最大化长期期望激励**的决策。

策略的效用：

![策略的效用_1.png](https://i.imgur.com/sO93akY.png)

决策的效用意味着当程序遵循了一种决策后，从指定点S0开始最后将得到的激励期望。

#### s 点效用奖励 U（s）与瞬时决策奖励 R（s）的区别

R(s)是当在s点状态时，将得到的**瞬时奖励**；
U(s)是在s点起遵循一种决策后，将得到的长期奖励的期望。也就是从s 点开始将获得**所有奖励的和**；

效用的意义：
效用恰恰映射了强化学习中延迟奖励的意义。
