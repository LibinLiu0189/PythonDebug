### 神经网络
每一个计算都是前一层的加权求和，再用`sigmoid`函数求值的结果，这些其他单元层通常称为隐藏层。

1. 人工神经网络是由感知机单元构成的，人工神经网络的输入应该是每行带有标签的数值型矩阵。

2. 从神经网络的输出中得到：
□ 一个有向图（神经网络本身）
□ 一个标量
□ 用向量表示的分类信息
□ 每个输入向量都对应一个输出向量

3. 

神经网络每个单元如下:

![神经网络](http://dataunion.org/wp-content/uploads/2015/03/115.png)
其对应的公式如下：

![神经网络公式](http://dataunion.org/wp-content/uploads/2015/03/27.png)

其中，该**单元**也可以被称作是**Logistic回归模型**。当将多个单元组合起来并具有分层结构时，就形成了神经网络模型。

![隐含层的神经网络](http://dataunion.org/wp-content/uploads/2015/03/342.png)

其对应的公式如下:

![对应的公式](http://dataunion.org/wp-content/uploads/2015/03/42.png)

#### 9 梯度下降
![梯度下降](http://img.blog.csdn.net/20131213085438093)

代码实现

![代码实现](http://img.blog.csdn.net/20131113202512453?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZG9uZ3Rpbmd6aGl6aQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

参考

[Logistic回归总结](http://blog.csdn.net/achuo/article/details/51160101 "Logistic回归总结")

度下降算法可以给我们提供一个求极值的方法。但是也会产生很多问题：
□ 局部的极值
□ 运行太耗时
□ 会产生无限次循环
□ 无法收敛


###### 反向传播 back propagation
核心思想:以对计算有益的方式来组织链式法则，对网络总所有的权值求导数。
计算神经网络的输出，使用正向传播算法。而神经网络的训练使用 反向传播算法（BP）。

#### 14 优化权重
**网络的复杂性**，不仅仅由节点和分层所决定，也和权值大小有关。

相同数量的权值、相同数量的节点、相同数量的分层，但权值过大也能增加网络的复杂性，已经出现过拟合的可能性。所以说有时不仅要通过赋予**更少的节点**或**分层**来改造网络，还可以通过将**值**保持在**合理的范围**来实现。

#### 15 限制偏差
##### 15.01 restriction bias意义:
1. 使用的任意一种数据结构所具有的表示能力。
2. 关于将要考虑的假设集合。

##### 15.02 神经网络中使用了哪些限制?
一个简单的感知器单元，是线性的，只考虑平面。
将`sigmoid`函数以及其它任意函数与节点结合起来,以便表达更多内容。只要有足够的隐藏层、单元，神经网络的限制很少。

##### 15.03 能表达连续函数吗？
可以考虑使用隐藏层，只要隐藏层有足够的隐藏单元，每个隐藏单元都可以代表需要建模函数的一部分，隐藏单元放在隐藏层，并在输出层中连接在一起，就可以构建任何连续的函数。

##### 15.04 如何避免过拟合？
权值过大可能导致过拟合。
当训练神经网络时，一般指定有限数量的隐藏单元以及有限数量的分层。并不是任何固定的神经网络，真的都可以得到任意函数。
**解决**:交叉验证。
在神经网络训练中，开始迭代过程，随着迭代的运行，误差实际上会不断的减小, 它对训练数据建模做的越来越好。
特别的，如果在某类留存测试集中或者在交叉验证集中看到这个误差，

![神经网络训练](https://i.imgur.com/mdtHObf.png)

开始比较高，然后下降，在某个点折返上升。在下降的点应该停止训练。

#### 15 喜好偏差 or 选择偏置
选择偏置：有关用来学习算法的内容，
比如，为什么选择一种不选择另一种等。
设置初始权值：
一般设置为小的随机值,意味着从低复杂性开始,优先选择更加简单的解释，原因如下：
1. 随机值有助于避免局部最优值；
2. 多次运行算法,如果停止，不希望它在那个点再次停止，如果停止了，可以再次运行算法，因此它可以赋予我们差异性，这在避免局部最优值上非常有用。

优先使用简单的算法，得到的泛化误差会更小。



