### 0.0 导论
1. 平方误差损失函数用于回归问题
2. 指数损失函数用于分类问题

----------

### 1.0 提升树
提升是一种机器学习技术，可以用于回归和分类的问题，它每一步产生弱预测模型(如决策树)，并加权累加到总模型中；如果每一步的弱预测模型的生成都是**依据损失函数的梯度**方式的，那么就称为梯度提升(Gradient boosting)

提升技术的意义：如果一个问题存在弱预测模型，那么可以通过提升技术的办法得到一个强预测模型。

#### 1.1 提升树模型
提升树模型可以表示为决策树的加法模型

![boost_tree_1.png](https://i.imgur.com/66eVCNJ.png)

其中，T(x;θ<sub>m</sub>)表示决策树；θ<sub>m</sub>表示决策树的参数；M为树的个数

#### 1.2 提升树算法
提升树算法采用前向分步法，首先确定初始提升树f<sub>0</sub>(x)=0，第m步的模型是

![boost_tree_2.png](https://i.imgur.com/kVHXfY6.png)

其中，f<sub>m-1</sub>(x)为当前模型，通过经验风险极小化确定下一颗决策树的参数θ<sub>m</sub>,

![boost_tree_3.png](https://i.imgur.com/wrpus6R.png)

#### 1.3 回归问题的提升树算法流程
输入：训练数据集T={(x<sub>1</sub>,y<sub>1</sub>),(x<sub>2</sub>,y<sub>2</sub>),...,(x<sub>N</sub>,y<sub>N</sub>)}；![boost_tree_4.png](https://i.imgur.com/eHNQkF5.png)

输出：提升树 f<sub>M</sub>(x)

(1) 初始化f<sub>0</sub>(x) = 0
(2) 对m=1,2,...,M</br>
(a) 计算残差

![boost_tree_5.png](https://i.imgur.com/g1b3VWb.png)

(b) 拟合残差r<sub>mi</sub>学习一个回归树，得到T(x;θ<sub>m</sub>)</br>
(c) 更新 f<sub>m</sub>(x) = f<sub>m-1</sub>(x) + T(x;θ<sub>m</sub>)

(3) 得到回归问题提升树：

![boost_tree_6.png](https://i.imgur.com/ausFV8O.png)

----------

### 2.0 多元分类GBDT算法详解

输入：训练数据集T={(x<sub>1</sub>,y<sub>1</sub>),(x<sub>2</sub>,y<sub>2</sub>),...,(x<sub>N</sub>,y<sub>N</sub>)}；![boost_tree_4.png](https://i.imgur.com/eHNQkF5.png)，损失函数L(y,f(x))

对于多元GBDT损失函数为：

![gbdt_r_2.png](https://i.imgur.com/fs13MI8.png)

其中p<sub>k</sub>(x):

![gbdt_r_3.png](https://i.imgur.com/zSONBBM.png)

输出：回归树 fhat(x)

(1) 初始化

![boost_tree_7.png](https://i.imgur.com/cMgPG5Z.png)

(2) 对m=1,2,...,M</br>
(a) i=1,2,...,N 计算

![boost_tree_8.png](https://i.imgur.com/O7J47PL.png)

推导过程参考：

![gbdt_r_1.png](https://i.imgur.com/5t91bWu.png)

(b) 对r<sub>mi</sub>拟合一个回归树，得到第m棵树的叶节点区域R<sub>mj</sub>，j=1,2,...,J；通常回归树需要满足低方差高偏差，而Cart树是理想的选择。</br>
(c) 对j=1,2,...,J，计算

![boost_tree_9.png](https://i.imgur.com/OVETFnF.png)

c<sub>mi</sub>是叶子节点的最佳残差拟合值。

第m轮的最佳拟合函数是：

![gbdt_r_4.png](https://i.imgur.com/4gfyyyn.png)

(d) 更新

![boost_tree_10.png](https://i.imgur.com/S4dKkYj.png)

(3) 得到回归树

![boost_tree_11.png](https://i.imgur.com/QQYyXYI.png)

### 3.0 Q&A
#### 3.1 GBDT优劣
优点：</br>
1. GBDT几乎可用于所有回归问题（线性/非线性），相对logistic regression仅能用于线性回归，GBDT的适用面非常广。亦可用于二分类问题（设定阈值，大于阈值为正例，反之为负例）。
1. 在相对少的调参时间情况下，预测的准备率也可以比较高。这个是相对SVM来说的。

缺点：</br>
1. 难以进行并行训练数据。

#### 3.2 RF与GBDT比较
1. RF中树的棵树是并行生成的；GBDT中树是顺序生成的；两者中过多的树都会过拟合，但是GBDT更容易过拟合；
2. RF中每棵树分裂的特征比较随机；GBDT中前面的树优先分裂对大部分样本区分的特征，后面的树分裂对小部分样本区分特征；
3. RF中主要参数是树的棵数；GBDT中主要参数是树的深度，一般为1；

#### 3.3 GBDT中树为什么必须是Cart?
因为Cart可以作为回归树。分类树用于分类标签值，其结果无法累加。

参考：
1. [GBDT理解二三事](https://blog.csdn.net/w28971023/article/details/43704775)

1. [刘建平：梯度提升树(GBDT)原理小结](https://www.cnblogs.com/pinard/p/6140514.html)

1. [机器学习系列1. GBDT算法的原理](https://blog.csdn.net/u012684933/article/details/51088609)

1. [GBDT算法推导过程](https://www.cnblogs.com/wuxiangli/p/5973737.html)

1. [Tree ensemble算法的特征重要度计算](https://blog.csdn.net/yangxudong/article/details/53899260)