### 01 集成学习的简单规则 
先通过某个数据子集进行学习，形成某个规则，然后通过另一个数据子集进行学习，形成不同的规则，接着通过另一个数据子集进行学习，形成第三个规则，接着更多；最后收集所有这些规则，并将他们合并成为复杂的规则。

#### 为什么考虑子集，而不考虑所有数据？
因为如果考虑所有数据的话，就很难在想到这些简单规则。

### 02.01 集成学习算法
挑选子集的时候遵守均匀规则。

### 02.02 集成Boosting
- 如何改变训练数据的权重或概率分布？
AdaBoost算法提高那些被前一轮弱分类器错误分类的样本的权重，而降低那些被正确分类的权重，这样做的好处是在下一轮的的分类过程中错误的分类由于权重加大而受到更大的关注。

- 如何将弱分类器组合为一个强分类器？
AdaBoost采用加权多数表决的方法。具体说来，即加大分类误差率小的弱分类器的权重，使其在表决中起较大作用，减小分类误差率较大的弱分类器的权重，使其在表决中起较小的作用。

### 03 AdaBoost简介
AdaBoost是adaptive boosting的缩写，其运行过程如下：

训练数据中的每个样本，并赋予一个权重，这些权重构成向量D。一开始，这些权重都初始化程相等值(`D = mat(ones((m, 1))/m)`)；

首先，在训练数据上训练出一个弱分类器并计算该分类器的错误率，

然后，在同一个数据集上再次训练弱分类器。在分类器的第二次训练当中，将会重新调整每个样本的权重，其中第一次分对的样本的权重将会降低，而第一次错分的权重将会提高。

为了从所有弱分类器中得到最终分类结果，Adaboost为每个分类器都分配一个权重值，这些alpha值是基于每个分类器错误率进行计算的。

其中，错误率的定义为：

`ε = 未正确分类的样本/所有样本`

alpha的计算公式：

`α = 0.5 * ln((1-ε) / ε)`

计算出alpha值之后，可以对权重向量D进行更新，以使得那些正确分类的样本的权重降低而错分样本的权重升高。D的计算方法如下：

如果某个样本被正确分类，那么该样本的权重更改为：

![adaboost_w_1.png](https://i.imgur.com/ZiINTOq.png)

如果某个样本被错误分类，那么该样本的权重更改为：

![adaboost_w_2.png](https://i.imgur.com/ROk6mTl.png)

在计算出D之后，Adaboost又开始进入下一轮迭代。AdaBoost算法会不断的重复训练和调整权重的过程，直到训练错误率为0或者弱分类器的数目达到用户的指定值为止。

#### 03.01 AdaBoost算法
![AdaBoost算法](https://i.imgur.com/JMf8LgI.png)

alpha （模型权重）目的主要是计算每一个分类器实例的权重(加和就是分类结果)
D （样本权重）的目的是为了计算错误概率： weightedError = D.T*errArr，求最佳分类器
  样本的权重值：如果一个值误判的几率越小，那么 D 的样本权重越小

#### 03.02 AdaBoost算法 伪代码
```
adaBoostTrainDS # 返回弱分类器集合 分类结果
    迭代次数
        1. buildStump # 最佳单层决策树
            # 循环所有的feature列，将列切分成 若干份，每一段以最左边的点作为分类节点
                for j in range(-1, int(numSteps)+1) # 步数
                     for inequal in ['lt', 'gt']: # go over less than and greater than
                         # 对单层决策树进行简单分类，得到预测的分类值
                # bestStump 表示分类器的结果，在第几个列上，用大于／小于比较，阈值是多少
            return bestStump| dim            表示 feature列
                            | threshVal      表示树的分界值
                            | inequal        表示计算树左右颠倒的错误率的情况
                   weightedError  表示整体结果的错误率
                   bestClasEst    预测的最优结果
        2. # 计算当前迭代最好的分类的alpha权重值
        3. 根据分类正确错误，更改样本权重
        4. # 预测的分类结果值，在上一轮结果的基础上，进行加和操作
           # aggClassEst += alpha*classEst(预测的最优结果)
        5. 计算错误率，如果小于预期，则使用新的样本权重继续迭代
```

### 04 AdaBoost总结
#### AdaBoost的优缺点：

- 优点
泛化错误率低，易编码，可以应用在大部分分类器上，无参数调整。

- 缺点
对离群点敏感。

- 其他
多个分类器组合可能会进一步凸显出单分类器的不足，比如过拟合问题

### 05 Bagging、Boosting二者之间的区别
#### 05.01.01 样本选择上：
- Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。
- Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。

#### 05.01.02 样例权重：
- Bagging：使用均匀取样，每个样例的权重相等。
- Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。

#### 05.01.03 预测函数：
- Bagging：所有预测函数的权重相等。
- Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。

#### 05.01.04 并行计算：
- Bagging：各个预测函数可以并行生成。
- Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。

#### 05.02 算法组合
Bagging + 决策树 = 随机森林
AdaBoost + 决策树 = 提升树
Gradient Boosting + 决策树 = GBDT
