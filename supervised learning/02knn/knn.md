### 00 knn
#### 00.01 算法原理
```
1.假设有一个带有标签的样本数据集（训练样本集），其中包含每条数据与所属分类的对应关系。
2.输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较。
    a.计算新数据与样本数据集中每条数据的距离。
    b.对求得的所有距离进行排序（从小到大，越小表示越相似）。
    c.取前 k （k 一般小于等于 20 ）个样本数据对应的分类标签。
3.求 k 个数据中出现次数最多的分类标签作为新数据的分类。
```
#### K值选择
k值越小表明模型越复杂，更加容易过拟合；越大且欠拟合。

k一般取一个较小的值，然后用过交叉验证来确定
1. 将样本划分一部分出来为预测样本，比如95%训练，5%预测，然后k分别取1，2，3，4，5之类的，进行预测；
2. 计算最后的分类误差，选择误差最小的k

#### 00.02 优缺点
KNN算法的优点：

1. 思想简单，理论成熟，既可以用来做分类也可以用来做回归；
1. 可用于非线性分类；
1. 训练时间复杂度为O(n)；
1. 准确度高，对数据没有假设，对outlier不敏感；

缺点：
1. 计算量大；
1. 样本不平衡问题（即有些类别的样本数量很多，而其它样本的数量很少）；
1. 需要大量的内存；

### 02.00 KNN偏差
**近似误差**，模型估计值与实际值之间的差距。
更关注于“训练”。

最小化近似误差，即为使估计值尽量接近真实值，但是这个接近只是对训练样本（当前问题）而言，模型本身并不是最接近真实分布。换一组样本，可能就不近似了。这种只管眼前不顾未来预测的行为，即为过拟合。

**估计误差**，模型的估计系数与实际系数之间的差距。
更关注于“测试”、“泛化”。

最小化估计误差，即为使估计系数尽量接近真实系数，但是此时对训练样本（当前问题）得到的估计值不一定是最接近真实值的估计值；但是对模型本身来说，它能适应更多的问题（测试样本）。

![M次多项式函数拟合问题的例子](https://pic1.zhimg.com/v2-d944c057e85de7c847dea5ca61ee6dc0_r.jpg)

如上图中对于这个训练集而言，其实选择3次多项式来作为预测模型是与实际模型最符合的，可是当选择9次多项式的话（对应k值越小），虽然对训练集的预测非常准确（近似误差越小），但是这是一个明显的过度拟合问题（overfitting），得出的预测模型的估计误差相对于3次多项式其实是更大的。

#### 02.01 曼哈顿距离
在平面上，坐标（x1, y1）的i点与坐标（x2, y2）的j点的曼哈顿距离为：
```
d(i,j)=|X1-X2|+|Y1-Y2|
```

#### 02.02 欧氏距离(欧几里得度量)
二维空间中的欧氏距离
```
0ρ = sqrt( (x1-x2)^2+(y1-y2)^2 )　|x| = √( x2 + y2 )
```

### 03 维度灾难
每次添加一个新特征时，就是将另一个维度加至你的输入空间，需要指数级的增加数据，从而可以进行准确的泛化。

1. 维度加权
2. 选择k,如果k=n,可以对较近的点加权

### 04 局部加权回归
![加权的J函数](http://img.blog.csdn.net/20160720154847377)

其中w(i)是权重，它根据要预测的点与数据集中的点的距离来为数据集中的点赋权值。当某点离要预测的点越远，其权重越小，否则越大。

一个比较好的权重函数如下：

![权重函数](http://img.blog.csdn.net/20160720155155460)

该函数称为指数衰减函数，其中**k**为波长参数，它控制了**权值随距离下降的速率**，该函数形式上类似高斯分布(正态分布)，但并没有任何高斯分布的意义。

参考

[局部加权回归](http://blog.csdn.net/herosofearth/article/details/51969517 "局部加权线性回归")

[如何理解和区分近似误差和估计误差?](https://www.zhihu.com/question/60793482 "如何理解和区分近似误差和估计误差?")