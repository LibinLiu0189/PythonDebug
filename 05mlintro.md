### 01 机器学习
强化学习:延时反馈
监督学习:函数逼近 泛化函数

### 02 测试集的好处
1. 检查过拟合
2. 评估分类器或回归在独立数据集上的性能

#### 02.01 针对sklearn中K折的实用建议
不应该用A类训练模型，然后预测B类

### 03 训练模型
**居中趋势测量**：均值、中值、众数。
**数据的离散性**：四分位距法、异常值、标准偏差、贝塞尔修正。

### 04 检测错误
**过拟合**：太过具体，训练集表现很好，倾向于记住非学习
较低的训练误差，较高的测试误差
**欠拟合**：较高的训练误差，测试误差

#### 04.01.01 训练 开发 测试集划分
数据集量比较小时的划分

![splitting_data_1.png](https://i.imgur.com/QWRZdiQ.png)

数据集量比较大时的划分

![splitting_data_2.png](https://i.imgur.com/D0LwoA3.png)

##### guidelines
- 设置测试集的大小，以对系统的整体性能给予高度的信心。
- 测试集有助于评估最终分类器的性能，该分类器可能占整体的`30%`。
- 验证集必须足够大，可以评估不同的想法。

#### 04.01.02 K折交叉验证
将训练集的`1/k`作为测试集，每个模型训练`k`次，测试`k`次，错误率为`k`次的平均，最终选择平均率最小的模型`Mi`。

##### 步骤
![K折交叉验证.png](https://i.imgur.com/XLiORJ0.png)

##### 注意
1. 训练、测试集的划分尽可能保持数据的一致性，避免因数据划分过程引入额外的偏差而对最终结果产生影响。
2. 不同的划分将导致不同的训练、测试集，相应的，模型评估的结果也会有差别，因此单次使用留出法得到的估计结果往往不够稳定可靠，在使用留出法时，一般要采用若干次随机划分、重复进行试验评估后取平均值作为留出法的评估结果。(使用网格搜索的依据)
3. 分层划分的窘境：如果训练集`S`包含绝大多数的样本，则训练模型可能更接近于用`D`训练的模型，但`T`比较小，评估结果可能不够准确；若令测试集`T`多包含一些样本，则训练集`S`与`D`差别更大了，从而降低评估结果的保真性。(模型评估与选择中用于评估测试数据集为验证集)



###### 实例
```
parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}
svr = svm.SVC()
clf = grid_search.GridSearchCV(svr, parameters)
clf.fit(iris.data, iris.target)
```
`parameters`参数字典以及他们可取的值。在这种情况下，他们在尝试找到 `kernel`（可能的选择为 '`linear`' 和 '`rbf`' ）和 `C`（可能的选择为1和10）的最佳组合。这时，会自动生成一个不同（`kernel`、`C`）参数值组成的“网格”, 各组合均用于训练 SVM，并使用交叉验证对表现进行评估。


**C类**似于正则化中`λ`的作用。`C`越大，拟合非线性的能力越强。 
`large C`: `High Variance`
`small C`: `High Bias`

```
clf.fit(iris.data, iris.target)
```
第二个不可思议之处。 拟合函数现在尝试了所有的参数组合，并返回一个合适的分类器，自动调整至最佳参数组合。现在您便可通过`clf.best_params_`来获得参数值。

#### 04.01.03 留一法
**留一法**其实就是样本量较小时使用的交叉验证，算是普通交叉验证的极端情况，即将所有N个样本分成N份，再进行交叉验证。

#### 04.02 网格搜索
网格搜索是用在选择模型参数的过程中。尝试所有的模型参数进行拟合，从中搜索到最佳的模型参数。之所以成为网格搜索，因为是在参数所建立的多维网格中找到最佳的参数点。

#### 04.03 K折交叉验证与网格搜索
网格搜索不使用交叉验证，使训练速度更快，但可能难以得到最优的模型参数；交叉验证对每一个参数组合得出的评分更为准确和鲁棒，提高评估的稳定性。

#### 04.04 学习曲线
![学习曲线](https://i.imgur.com/vFRX1dP.png)

- 欠拟合，随着训练集增加，训练误差和测试误差接近。
- 良好拟合，随着训练集增加，训练误差和测试误差接近，此时误差较欠拟合低。
- 过拟合，随着训练集增加，训练误差和测试误差接近，但是训练误差和测试误差相差较欠拟合大一些。交叉误差始终不会太低。

#### 04.05 模型复杂度
与学习曲线图形不同，**模型复杂度**图形呈现的是模型复杂度如何改变训练曲线和测试曲线，而不是呈现用来训练模型的数据点数量。一般趋势是，**随着模型增大，模型对固定的一组数据表现出更高的变化性**。

#### 04.06 使用RandomizeSearchCV来降低计算代价¶
- RandomizeSearchCV用于解决多个参数的搜索过程中计算代价过高的问题
- RandomizeSearchCV搜索参数中的一个子集，这样你可以控制计算代价 

[网格搜索来进行高效的参数调优](http://blog.csdn.net/jasonding1354/article/details/50562522 "网格搜索来进行高效的参数调优")

### 06 评估指标
#### 06.01 混淆矩阵
![混淆矩阵](https://i.imgur.com/K7TfktM.png)

#### 06.02 准确率
分类正确的(真阳真阴)/总树木

#### 06.03 准确率不适用的情形
可能会忽略一些点。

比如要找到信用卡不良的记录。

#### 06.05.01 精度和召回率
1. 对医疗来说，假阴不可接受，假阳可以。(高召回率)

因为健康的人可以识别为有病，但有病不能识别为健康。

2. 对垃圾邮件来说，假阴可以接受，假阳却不能。(高精度)

因为垃圾邮件可以接受，但正确邮件不能丢失。

#### 06.05.02 精度和召回率 计算
![分类结果混淆矩阵](https://i.imgur.com/rpsORrV.png)

###### 06.05.03 精确率
实际上非常简单，**精确率**(`precision`)是针对我们预测结果而言的，它表示的是预测为正的样本中有多少是真正的正样本。那么预测为正就有两种可能了，一种就是把正类预测为正类(`TP`)，另一种就是把负类预测为正类(`FP`)，也就是

![精度率](https://www.zhihu.com/equation?tex=P++%3D+%5Cfrac%7BTP%7D%7BTP%2BFP%7D)

###### 06.05.04 召回率

而**召回率**(`recall`)是针对我们原来的样本而言的，它表示的是样本中的正例有多少被预测正确了。那也有两种可能，一种是把原来的正类预测成正类(`TP`)，另一种就是把原来的正类预测为负类(`FN`)。

![召回率](https://www.zhihu.com/equation?tex=R+%3D+%5Cfrac%7BTP%7D%7BTP%2BFN%7D)

其实就是分母不同，一个分母是预测为正的样本数，另一个是原来样本中所有的正样本数。

![精确率和召回率](https://pic1.zhimg.com/80/d701da76199148837cfed83901cea99e_hd.jpg)

在信息检索领域，精确率和召回率又被称为查准率和查全率，
查准率＝检索出的相关信息量 / 检索出的信息总量
查全率＝检索出的相关信息量 / 系统中的相关信息总量

###### 06.05.05 精确率 召回率 模型评估
![P-R曲线与平衡点示意图](https://i.imgur.com/NcLw4t2.png)

#### 06.08 F1得分
![F1得分](https://i.imgur.com/3A8Y6mQ.png)

#### 06.09 Fβ得分
![fβ得分](https://i.imgur.com/B0cj2yH.png)

- β越小越偏向于精度，反之召回。
```
1 对于宇宙飞船，我们不允许出现任何故障零件，可以检查本身能正常运转的零件。因此，这是一个高召回率模型，因此 β = 2。
2 对于通知模型，因为是免费发送给客户，如果向更都的用户发送邮件也无害。但是也不能太过了，因为可能会惹怒用户。我们还希望找到尽可能多感兴趣的用户。因此，这个模型应该具有合适的精度和合适的召回率。β = 1 应该可行。
3 对于促销材料模型，因为发送材料需要成本，我们不希望向不感兴趣的用户发送材料。因此是个高精度模型。β = 0.5 应该可行。
```
######  06.09.02 F-β得分的界限
![fβ得分的界限](https://i.imgur.com/xCinhF3.png)

#### 06.10 ROC曲线
根据机器学习的预测结果对样例进行排序，按此顺序逐个把样本作为正例进行预测，每次计算两个重要量的值，分别以它们为横纵坐标作图，得到ROC曲线。

![真正例率_假正例率](https://i.imgur.com/VUEJxW6.png)

![roc_auc_1.png](https://i.imgur.com/zLakro5.png)

曲线坐标上：

X轴是FPR（表示假阳率-预测结果为positive，但是实际结果为negitive，FP/(N)）
Y轴式TPR（表示真阳率-预测结果为positive，而且的确真实结果也为positive的，TP/P）

那么平面的上点(X,Y)：

(0,1)表示所有的positive的样本都预测出来了，分类效果最好
(0,0)表示预测的结果全部为negitive
(1,0)表示预测的错过全部分错了，分类效果最差
(1,1)表示预测的结果全部为positive

针对落在x=y上点，表示是采用随机猜测出来的结果。

#### 06.10.01 Auc
`ROC`曲线下面的面积叫做`AUC`。
一般情况下ROC会在x=y的上方，所以0.5<AUC<1。AUC越大说明分类效果越好。

~~##### 灵敏度 特异度~~
~~灵敏度=真阳性人数/（真阳性人数+假阴性人数）*100%。~~
~~表示的是所有正例中被分对的比例，衡量了**分类器对正例的识别能力**。~~

~~特异度=真阴性人数/（真阴性人数+假阳性人数））*100%。正确判断非病人的率。~~
~~表示的是所有负例中被分对的比例，衡量了**分类器对负例的识别能力**。~~

参考:
[准确率(Precision)、召回率(Recall)以及综合评价指标(F1-Measure)](https://blog.csdn.net/sruixue/article/details/40076271 "title")
#### 06.10.02 拟合程度与ROC曲线

![ROC曲线](https://i.imgur.com/UhjXxPI.png)

总结：**AUC曲线面积越接近于1,模型就越好**

#### 06.10.03 判断ROC曲线优劣
方法：比较ROC下的面积

![auc_1](https://i.imgur.com/cXognWE.png)

ROC和AUC通常是用来评价一个二值分类器的好坏。
#### 06.10.04 如何绘制RoC曲线？
`FPR（False Positive Rate）`和`TPR（True Positive Rate）`分别对应着`ROC`曲线的横纵坐标。但是学习器它只能绘制一个点，不足以绘制出一条曲线。实际上对于许多学习器在判定二分类问题时是预测出一个对于真值的范围在[0.0, 1.0]之间的概率值，而判定是否为真值则看该概率值是否大于设定的阈值（`Threshold`）。

例如如果阈值设定为0.5则所有概率值大于0.5的均为正例，其余为反例。因此对于不同的阈值我们可以得到一系列相应的`FPR`和`TPR`，从而绘制出`ROC`曲线。 

因此，选取多组阈值，计算对应的`FPR`和`TPR`，然后做图。

#### 06.10.05 Gini Auc
```
Gini=2*AUC-1
```

![gini](https://camo.githubusercontent.com/3c965c724ef6c7d0a40e57bb0692e3d8a7bdfbb8/68747470733a2f2f706963332e7a68696d672e636f6d2f64653066356434633734336132326638393138313036323336613938633332635f722e6a7067)

参考：[gini系数](https://github.com/le773/PythonDebug/blob/b517ea57d6f830f09d9dc7809d5242a2c6408230/regression/treereg.md "gini系数")

#### 06.10.06 方差 偏差 窘境
![泛化误差_方差_偏差](https://i.imgur.com/z0x9SL9.png)

1. 训练不足时，学习器拟合能力不够强，训练数据的扰动不足以使学习器产生显著变化，此时偏差主导泛化错误；

2. 随着训练程度的加深，学习器拟合能力逐渐增强，训练数据发生的扰动渐渐能被学习器学到，方差主导了泛化错误率;

3. 在训练程度充足后，学习器的拟合能力已经非常强，训练数据发生的轻微扰动都会导致学习器发生显著变化，若训练数据自身的、非全局的特性被学习器学到，则发生过拟合。


#### 06.11 决定系数 R2 
`R2`的数值范围从`0`至`1`，表示目标变量的预测值和实际值之间的**相关程度**平方的百分比。

一个模型的`R2` 值为0还不如直接用平均值来预测效果好；而一个`R2` 值为1的模型则可以对目标变量进行完美的预测。从0至1之间的数值，则表示该模型中目标变量中有百分之多少能够用特征来解释。模型也可能出现负值的`R2`，这种情况下模型所做预测有时会比直接计算目标变量的平均值差很多。

```
def performance_metric2(y_true, y_predict):
    """计算并返回预测值相比于预测值的分数"""
    y_mean = sum(y_true)/len(y_true)
    sst = sum(map(lambda x:(x-y_mean)**2, y_true))
    ssr = sum([(x-y)**2 for x, y in zip(y_true, y_predict)])
    score = 1- ssr/sst
    return score
```

![决定系数](https://i.imgur.com/Hq15eSP.png)

- ssr:(训练模型得到的结果-平均值) *(训练模型得到的结果-平均值)之和
- sst:(实际值-平均值) *(实际值-平均值)之和
- sse:(实际值-训练模型得到的结果)*(实际值-训练模型得到的结果)之和

参考：[决定系数](https://en.wikipedia.org/wiki/Coefficient_of_determination)

#### 06.12 可避免偏差 方差
训练集的误差和人类表现水平的误差的差值就称为可避免偏差（`Avoidable Bias`）。

![human-level_training-error_dev-error.png](https://i.imgur.com/VZhA3hV.png)

1. 如果人类表现水平和训练误差 大于 验证集和训练集的误差，则降低可避免误差。
2. 如果人类表现水平和训练误差 小于 验证集和训练集的误差(不能很好的泛化)，则降低方差。

#### 07.01 进行误差分析
如果希望让学习算法能够胜任人类能做的任务，但学习算法还没有达到人类的表现，那么人工检查一下算法犯的错误，可以了解接下来应该做什么，这个过程称为**误差分析**。

![dev_evaluate_ideas_1.png](https://i.imgur.com/ogyltrk.png)

假设在猫狗分类中，存在10%的出错率，将一些看起来像猫的狗识别为猫，此时，可以人工选取100个样本，然后识别其中有多少个是狗，假设包含5只狗被识别为猫，那么可以推断，出错率可以降低至9.5%，所以为了降低出错率，重新构建一个项目训练是不合理的。这样，**通过少量的时间去分析问题，再决定后面要进行大方向**。

![dev_evaluate_ideas_2.png](https://i.imgur.com/Rl7OB6P.png)

这个分析步骤的结果可以给出一个估计，是否值得去处理每个不用的错误类型，筛选出高优先级任务，并了解每种手段对性能能有多大的提升空间。

总结：进行误差分析，应该找一组错误例子，可能在验证集、测试集，观察错误标记的例子，看看假阳性和假阴性，统计属于不同错误类型的错误数量，在这个过程中，可能得到启发，归纳出新的错误类型。总之，通过统计不同错误标记类型占总数的百分比，可以帮你发现那些问题需要优先解决。

#### 07.02 清楚标注错误的数据

事实证明，深度学习算法对于**训练集中的随机误差是相当鲁棒**的，只要标记错误例子离随机误差不太远，有时可能做标记不小心等，只要误差够随机，那么这些误差不管也没有问题。

验证集、测试集：如果这些标记错误严重影响在开发集上评估算法的能力，那么就应该花时间修正错误的标签，如果没，则不。

![dev_evaluate_ideas_3.png](https://i.imgur.com/LaOqSI7.png)

- 确保测试集、验证集来自同一个分布，如果对测试集矫正，那么验证集也需要。
- 同时检查算法训练正确和错误的样本。
- 训练集可以来自和测试集、验证集稍微不同的分布。

#### 07.03 快速搭建第一个系统原型，然后快速迭代
Guideline:
`Build your first system quickly, then iterate`
在构建一个机器学习系统时，尽可能按照以下几点来做：
1. 设置好训练、开发、测试集及衡量指标，找准目标；
2. 快速构建出一个初步的系统，用训练集来拟合参数，开发集调参，测试集评估；
3. 采用方差/偏差分析或者错误分析等方法来决定下一步工作。

#### 07.04 在不同的划分上进行训练并测试
![dev_evaluate_ideas_4.png](https://i.imgur.com/hbgcu7y.png)

1. 对于`option1`，测试集包含少量来及`app`集合，那么其实训练的模型预测`web`集合更好，这与预测`app`集相悖。
2. 对于`option2`，测试集、验证集全部来自`app`，那么训练的模型其实是预测来自`app`集的。

总结：构建机器学习模型是，在验证集、测试集上，一定要反映出将来需要面临的数据。

#### 07.05 不匹配数级划分的偏差和方差
**背景**：当训练集来自和验证集、测试集不同的分布时，分析偏差、方差的方式可能不一样。当来自不同分布时，很难确定导致问题的根源。

采取随机打散训练集，然后分出一部分训练集作为**训练-验证集**，这部分不会用来训练模型。

![dev_evaluate_ideas_5.png](https://i.imgur.com/3hmDYog.png)

1. 训练验证误差 - 验证误差 > 训练误差 - 训练验证误差时，因为训练误差、训练验证误差来自同一分布的数据中测得的，尽管神经网络在训练集表现良好，但无法泛化来自相同分布的训练-验证集。
2. 训练验证误差 - 验证误差 < 训练误差 - 训练验证误差时，则数据分布不匹配。
3. 当`Bayes-error - Training-error > Training-error - Dev-Training-error`时，则是`Avoidable Bias`

#### 07.06 定位数据不匹配
![data_mismatch_1.png](https://i.imgur.com/ThvN8WE.png)

在一些情况下，可以直接通过一些手段直接解决数据集不匹配的问题。

首先，需要进行人工错误分析，以了解训练、开发、测试集之间存在的差异，

随后根据分析结果制作一些数据或者收集之间互相匹配的数据集。

例如，必要是可以进行人工合成，将一些高质量的照片模糊化，或者对一些声音信息加入一些噪音，以此来使数据集匹配。


#### 07.07 在分类中如何处理训练集中不平衡问题？

A. 可以扩大数据集吗？ 

当遇到类别不均衡问题时，首先应该想到，是否可能再增加数据（一定要有小类样本数据），**更多的数据往往战胜更好的算法**。因为机器学习是使用现有的数据多整个数据的分布进行估计，因此更多的数据往往能够得到更多的分布信息，以及更好分布估计。即使再增加小类样本数据时，又增加了大类样本数据，也可以使用放弃一部分大类数据（即对大类数据进行欠采样）来解决。

B. 尝试其它评价指标 

1. 混淆矩阵
1. 精确度
1. 召回率
1. F1得分
1. ROC曲线

C. 对数据集进行重采样 

可以使用一些策略该减轻数据的不平衡程度。 采样算法往往很容易实现，并且其运行速度快，并且效果也不错。

1. 对小类的数据样本进行采样来增加小类的数据样本个数，即**过采样**(`over-sampling` ，采样的个数大于该类样本的个数)。
1. 对大类的数据样本进行采样来减少该类数据样本的个数，即**欠采样**(`under-sampling`，采样的次数少于该类样本的个素)。

D. 尝试产生人工数据样本 

E. 尝试不同的分类算法(集成方法)

F. 尝试对模型进行惩罚 

通过正负样本的惩罚权重解决样本不均衡的问题的思想是在算法实现过程中，对于分类中不同样本数量的类别分别赋予不同的权重（一般思路分类中的小样本量类别权重高，大样本量类别权重低），然后进行计算和建模。

G:通过特征选择解决样本不均衡

如果小类别样本量具有一定的规模，那么意味着其特征值的分布较为均匀，可通过选择具有显著型的特征配合参与解决样本不均衡问题，也能在一定程度上提高模型效果。

#### 07.08 如果训练样本数量少于特征数量，怎么办？
如果训练集很小，那么高偏差/低方差分类器（如朴素贝叶斯分类器）要优于低偏差/高方差分类器（如k近邻分类器），因为高维特征空间意味着更高的数据过拟合风险。(如医学领域)

然而，随着训练集的增大，低偏差/高方差分类器将开始胜出（它们具有较低的渐近误差），因为高偏差分类器不足以提供准确的模型。

#### 07.09 生成模型和判别模型区别?
监督学习分为生成模型和判别模型。

生成模型：由数据学习联合概率分布P(x,y)，然后求出条件概率分布P(y|x)作为预测的模型，即生成模型P(Y|X)= P(X,Y)/P(X)。给定x产生出y的生成关系。</br>
eg：朴素贝叶斯、隐马尔科夫

判别模型：由数据直接学习决策函数f(x)或者条件概率分布P(y|x)作为预测模型。给定x，应该预测什么样的输出y。</br>
eg：KNN、感知机、决策树、逻辑斯蒂回归、最大熵、svm、提升方法、条件随机场

两者关系：</br>
由生成模型可以得到判别模型，但由判别模型得不到生成模型。

#### 08.01 迁移学习
`Transfer learning`，顾名思义就是就是把已学训练好的模型参数迁移到新的模型来帮助新模型训练。

参考：

1. [机器学习实践中的 7 种常见错误](http://blog.jobbole.com/70684/)

1. [在分类中如何处理训练集中不平衡问题](https://blog.csdn.net/heyongluoyao8/article/details/49408131 "在分类中如何处理训练集中不平衡问题")

1. [机器学习中非均衡数据集的处理方法？](https://www.zhihu.com/question/30492527)

1. [判别模型与生成模型](https://zhuanlan.zhihu.com/p/32655097)

1. [生成模型与判别模型](https://blog.csdn.net/zouxy09/article/details/8195017)